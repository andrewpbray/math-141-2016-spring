---
title: "Simple Linear Regression II"
output:
  ioslides_presentation:
    incremental: true
---

```{r setup, include=FALSE}
library(knitr)
options(digits=3)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(oilabs)
library(openintro)
```

## 
<center>
<img src="../figs/id-the-slr.png" width = 700>
</center>


## 
<center>
<img src="../figs/res-plots1.png" width = 700>
</center>


## Estimating $\beta_1$ {.build}

```{r find-b1, echo = FALSE}
poverty <- read.delim("../poverty.txt", header = TRUE)
library(ggplot2)
(p1 <- ggplot(poverty, aes(Poverty, Graduates)) + 
  xlim(0, 20) +
  ylim(75, 96) +
  geom_point())
```

We use $s_x, s_y, \textrm{ and } R$ to calculate $b_1$.


## Estimating $\beta_1$ {.build}

```{r find-b1-2, echo = FALSE}
m1 <- lm(Graduates ~ Poverty, data = poverty)
(p2 <- p1 + 
   geom_abline(intercept = 93, slope = m1$coef[2], col = "goldenrod") +
   annotate("text", x = 3, y = 87, label = paste("slope = ", round(m1$coef[2], 2))))
```

We use $s_x, s_y, \textrm{ and } R$ to calculate $b_1$.


## Estimating $\beta_0$ {.build}

```{r find-b02, echo = FALSE}
x_bar <- mean(poverty$Poverty)
y_bar <- mean(poverty$Graduates)
p1 + 
  geom_vline(xintercept = mean(poverty$Poverty), 
                col = "steelblue",
                lty = 2) +
  geom_hline(yintercept = mean(poverty$Graduates), 
                col = "steelblue",
                lty = 2) + 
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2], col = "goldenrod") +
  annotate("text", x = 3, y = 91, label = paste("slope = ", round(m1$coef[2], 2))) +
  annotate("text", x = 14, y = 96, label = paste("x-bar = ", round(x_bar, 2))) +
  annotate("text", x = 19, y = 85, label = paste("y-bar = ", round(y_bar, 2)))
```

If the line of best fit *must* pass through $(\bar{x}, \bar{y})$, what is $b_0$?


## Estimating $\beta_0$ {.build}

Since $(11.35, 86.01)$ is on the line, the following relationship holds.

$$ 86.01 = b_0 - 0.9 (11.35) $$

Then just solve for $b_0$.

$$ b_0 = 86.01 + 0.9 (11.35) = 96.22$$

More generally:

$$ b_0 = \bar{y} - b_1 \bar{x} $$


## Estimation in R {.smaller .build}

```{r fitlm}
m1 <- lm(Graduates ~ Poverty, data = poverty)
summary(m1)
```


## The `lm` object {.build .smaller}

```{r showlm}
attributes(m1)
m1$coef
m1$fit
```


## Interpretation of $b_1$ {.build}

The **slope** describes the estimated difference in the $y$ variable if the explanatory
variable $x$ for a case happened to be one unit larger.

```{r}
m1$coef[2]
```

*For each additional percentage point of people living below the poverty level,
we expect a state to have a proprotion of high school graduates that is 0.898
lower*.

**Be Cautious**: if it is observational data, you do not have evidence of a 
*causal link*, but of an association, which still can be used for prediction.


## Interpretation of $b_0$ {.build}

The **intercept** is the estimated $y$ value that will be taken by a case with 
an $x$ value of zero.

```{r}
m1$coef[1]
```

While necessary for prediction, the intercept often has no meaningful interpretation.


# Outliers

## What is an outlier? {.build}

<div class="columns-2">
![outliers](http://marcoghislanzoni.com/blog/wp-content/uploads/2013/10/outliers_gladwell.jpg)

**Outlier** is a general term to describe a data point that doesn't follow the
pattern set by the bulk of the data, when one takes into account the model.
</div>


## Outlier Example One

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# this chunk sets the chunk options for the whole document
require(knitr)
opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r, echo=FALSE}
library(openintro)
COL <- c('#55000088','#225588')
set.seed(238)
n <- c(50, 25, 78, 55, 70, 150)
m <- c(12, -4, 7, -19, 0, 40)
xr <- list(0.3, c(2), 1.42, runif(4,1.45,1.55), 5.78, -0.6)
yr <- list(-4, c(-8), 19, c(-17,-20,-21,-19), 12, -23.2)
i <- 1
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x, y, col = COL[2], lCol = COL[1], lwd = 3)
```


## Outlier Example Two

```{r, echo=FALSE}
i <- 2
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x, y, col = COL[2], lCol = COL[1], lwd = 3)
```


## Outlier Example Three

```{r, echo=FALSE}
i <- 3
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x, y, col = COL[2], lCol = COL[1], lwd = 3)
```


## Outlier Example Four

```{r, echo=FALSE}
i <- 5
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x, y, col = COL[2], lCol = COL[1], lwd = 3)
```


## Outlier Example Four

```{r, echo=FALSE}
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x[1:70], y[1:70], col = COL[2], lCol = COL[1], lwd = 3, xlim = range(x), ylim = range(y))
```


## Outliers, leverage, influence {.build}

**Outliers** are points that don't fit the trend in the rest of the data.

**High leverage points** have the potential to have an unusually large influence 
on the fitted model.

**Influential points** are high leverage points that cause a very different
line to be fit than would be with that point removed.


## Example of high lev, high influence

We have data on the surface temperature and light intensity of 47 stars in the
star cluster CYG OB1, near Cygnus.

```{r, echo=FALSE}
library(faraway)
data(star)
par(mar=c(4,4,2,1), las=1, mgp=c(2.5,0.7,0), cex.lab = 1.25, cex.axis = 1.25)
plot(light ~ temp, data = star, pch=19, col=COL[2], xlab = "log(temp)", ylab = "log(light intensity)")
```


## Example of high lev, high influence

We have data on the surface temperature and light intensity of 47 stars in the
star cluster CYG OB1, near Cygnus.

```{r, echo=FALSE}
par(mar=c(4,4,2,1), las=1, mgp=c(2.5,0.7,0), cex.lab = 1.25, cex.axis = 1.25)
plot(light ~ temp, data = star, pch=19, col=COL[2], xlab = "log(temp)", ylab = "log(light intensity)")
abline(lm(light~temp, data = star), col = "darkgreen", lwd = 3, lty = 2)
legend("top", inset = 0.05, "w/ outliers", lty = 2, lwd = 2, col = "darkgreen")
```


## Example of high lev, high influence

We have data on the surface temperature and light intensity of 47 stars in the
star cluster CYG OB1, near Cygnus.

```{r, echo=FALSE}
par(mar=c(4,4,2,1), las=1, mgp=c(2.5,0.7,0), cex.lab = 1.25, cex.axis = 1.25)
plot(light ~ temp, data = star, pch=19, col=COL[2], xlab = "log(temp)", ylab = "log(light intensity)")
abline(lm(light~temp, data = star), col = "darkgreen", lwd = 3, lty = 2)
abline(lm(light[temp>4]~temp[temp>4], data = star), col = COL[1], lwd = 3)
legend("top", inset = 0.05, c("w/ outliers","w/o outliers"), lty = c(2,1), lwd = c(2,3), col = c("darkgreen",COL[1]))
```


## Example of high lev, low influence

```{r, echo=FALSE}
set.seed(12)
i <- 2
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
y <- y - mean(y)
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x, y, col = COL[2], lCol = COL[1], lwd = 3)
```


## From leverage to influence

**Leverage** measures the weight given to each point in determining the regression
line.

**Influence** measures how different the regression line would be without a given
point. Often measured with *Cook's Distance*.

```{r, echo=FALSE, fig.height=4, }
i <- 5
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mfrow = c(1, 2))
plot(x, y, col = COL[2], pch = 16)
abline(lm(y ~ x), col = COL[1], lwd = 3)
x2 <- x[1:70]
y2 <- y[1:70]
plot(x2, y2, col = COL[2], pch = 16, xlim = range(x), ylim = range(y))
abline(lm(y2 ~ x2), col = COL[1], lwd = 3)
```


##

In the following plots are there outliers, leverage pts, or influential pts?

<center>
<img src="../figs/outliers.png" height = 500>
</center>
