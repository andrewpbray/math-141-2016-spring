---
title: "MATH 141"
author: "Chester Ismay"
output:
  ioslides_presentation:
    fig.align: center
    keep_md: yes
    logo: ../figs/griffin.png
    widescreen: yes
subtitle: Experimental Design
---

```{r setup, include=FALSE}
pkg <- c("dplyr", "readr", "mosaic")

new.pkg <- pkg[!(pkg %in% installed.packages())]

if (length(new.pkg)) {
  install.packages(new.pkg, repos = "http://cran.rstudio.com")
}

lapply(pkg, library, character.only = TRUE)
#setwd("/Users/cismay/Google Drive/ismayc.github.io")
```

# Simple Random Sampling versus Stratified Sampling

## 2015 MLB player salaries

Let's investigate the average salary of four teams:
  - Los Angeles Dodgers
  - New York Yankees
  - Tampa Bay Devil Rays
  - Arizona Diamondbacks

```{r load_data, include=FALSE}
url <- "http://www.usatoday.com/sports/mlb/salaries/"
salaries <- read_csv("mlb_salaries.csv") %>%
  select(NAME, TEAM, POS, SALARY)
salaries_trim <- salaries %>% 
  filter(TEAM %in% c("LAD", "NYY", "TB", "ARI"))
salaries_summary <- salaries_trim %>% group_by(TEAM) %>%
  summarize(mean_sal = mean(SALARY),
    count_team = n())
grand_mean <- salaries %>% summarize(mean(SALARY))
```

```{r}
salaries_summary <- salaries %>% group_by(TEAM) %>%
  summarize(mean_sal = mean(SALARY),
    count_team = n())
grand_mean <- salaries %>% summarize(mean(SALARY))
```


- `r nrow(salaries_trim)` player salaries available for those four teams
- The mean salary for ALL `r nrow(salaries_trim)` players on those four teams is $$\mu = \$`r prettyNum(grand_mean, big.mark=",", scientific=FALSE)`$$

- Remember, this is a summary value for the **population**.  We hardly ever have the whole population to work with.


```{r include=FALSE}
teams <- c(rep("yankees", 40), rep("padres", 40), rep("mariners", 40), rep("dodgers", 40))
salary <- c(rnorm(40, mean = 25), rnorm(40, 16), rnorm(40, 23), rnorm(40, 15))
df <- data.frame(teams, salary)
head(df)
n <- 40
```

## How do SRS and Stratified Sampling compare in estimating $\mu$?

- Suppose we select `r n` players at random from the `r nrow(salaries_trim)` total

```{r}
set.seed(20160129)
mean_srs <- salaries %>% 
  sample_n(n) %>%
  summarize(mean_srs_salary = mean(SALARY))
strat_n <- 10
```

$\bar{x}_{SRS} = \$`r prettyNum(mean_srs, big.mark=",", scientific=FALSE)`$

## Now for Stratified

- Let's select `r strat_n` players from each of the 4 teams

```{r sim}
mean_strat_by_team <- salaries %>% group_by(TEAM) %>%
  sample_n(strat_n) %>%
  summarize(mean_by_team = mean(SALARY))
mean_strat <- mean_strat_by_team %>%
  summarize(mean(mean_by_team))
```

$\bar{x}_{STRAT} = \$`r prettyNum(mean_strat, big.mark = ",", scientific=FALSE)`$

SRS: Absolute bias of \$`r prettyNum(abs(grand_mean - mean_srs), big.mark = ",", scientific=FALSE)`

Stratified: Absolute bias of \$`r prettyNum(abs(grand_mean - mean_strat), big.mark = ",", scientific=FALSE)`

### So is this evidence that simple random sampling is clearly better?

## Not so fast!

```{r comp, echo=FALSE, fig.align="center", fig.height=4}
srs_mean <- function(df){
  df %>% 
    summarize(mean_srs_salary = mean(SALARY))
} 

strat_mean <- function(df){
  mean_strat_by_team <- df %>% group_by(TEAM) %>%
    sample_n(strat_n) %>%
    summarize(mean_by_team = mean(SALARY))
  mean_strat <- mean_strat_by_team %>%
    summarize(mean(mean_by_team))
  mean_strat
}

# sample repeatedly (1000 times) 150 rows of df and store in a list
# apply the custom function to each sample in the list,
# bind rows together and create an index column, all in a "pipe":

srs_sim <- replicate(1000, sample_n(salaries, n), simplify = FALSE) %>%
  lapply(., srs_mean) %>% 
  bind_rows %>%
  mutate(replicate = 1:n())
  

strat_sim <- replicate(1000, sample_n(group_by(salaries, TEAM), strat_n), simplify = FALSE) %>%
  lapply(., strat_mean) %>% 
  bind_rows
  

# Comparison

#SRS <- do(1000) * mean(~salary, data = df)
#STR <- do(1000) * mean(~salary, data = rbind(sample(subset(df, teams == "yankees"), 10),
#                                             sample(subset(df, teams == "padres"), 10),
#                                             sample(subset(df, teams == "mariners"), 10),
#                                             sample(subset(df, teams == "dodgers"), 10)))
sim <- bind_cols(srs_sim, strat_sim) %>%
  select(replicate, everything())

names(sim) <- c("replicate", "SRS", "STR")
densityplot(~SRS + STR, data = sim, auto.key = TRUE)
#sim %>% ggplot(aes)

```

#

## {.flexbox .vcenter}

<img src="../figs/corrs1.png" alt="corrs1" width="850">


## {.flexbox .vcenter}

<img src="../figs/corrs2.png" alt="corrs2" width="850">


## If you learn one thing in this class... {.flexbox .vcenter}

<img src="../figs/xkcd-correlation.png" alt="corr" width="850">

## Principles of Experimental Design {.build}

**Control**: Compare treatment of interest to a control group.

**Randomization**: Randomly assign subjects to treatments.

**Replication**: Within a study, replicate by collecting a sufficiently large sample. Or replicate the entire study.

**Blocking**: If there are variables that are known or suspected to affect the response variable, first group subjects into blocks based on these variables, and then randomize cases within each block to treatment groups.


## Replication

<img src="../figs/psych.png" alt="psych" width="750">


## Blocking

A study is designed to test the effect of light level and noise level on exam performance of students. The researcher also believes that light and noise levels might have different effects on males and females, so wants to make sure both genders are represented equally under different conditions. Which of the below is correct?

1. There are 3 explanatory variables (light, noise, gender) and 1 response variable (exam performance)
2. There are 2 explanatory vars (light and noise), 1 blocking var (gender), and 1 response var (exam performance)
3. There is 1 explanatory var (gender) and 3 response vars (light, noise, exam performance)
4. There are 2 blocking vars (light and noise), 1 explanatory var (gender), and 1 response var (exam performance)


## Other key ideas {.build}

**Placebo**: fake treatment, often used as the control group for medical studies

**Placebo effect**: experimental units showing improvement simply because they believe they are receiving a special treatment

**Blinding**: when experimental units do not know whether they are in the control or treatment group

**Double-blind**: when both the experimental units and the researchers do not know who is in the control and who is in the treatment group


## Consider acupuncture {.build}

<img src="../figs/acupuncture.png" alt="acupuncture" width="500">

How do you test if acupuncture reduces pain?

"Sham acupuncture" is a good control.


# Practice

## Practice

1. Find your numerical pair
2. Introduce yourself (name, year, major, hometown)
3. Discuss the problems on the handout and record your thoughts.

